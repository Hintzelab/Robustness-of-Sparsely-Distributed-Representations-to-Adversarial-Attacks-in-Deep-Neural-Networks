{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)  # Flatten the input images\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the transformations to be applied to the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset with the defined transformations\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the neural network\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the neural network\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:    # Print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted_test = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted_test == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    test_acc = 100 * correct_test / total_test\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Plot the training and test accuracy\n",
        "epochs = range(1, 11)\n",
        "plt.plot(epochs, train_acc_list, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, test_acc_list, 'r', label='Test accuracy')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1eVv_0SBKmYm",
        "outputId": "a2563990-da09-4506-909c-4097b3cb6e09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 2.171\n",
            "[1,   200] loss: 1.946\n",
            "[1,   300] loss: 1.772\n",
            "[1,   400] loss: 1.647\n",
            "[1,   500] loss: 1.524\n",
            "[1,   600] loss: 1.443\n",
            "[1,   700] loss: 1.375\n",
            "[1,   800] loss: 1.316\n",
            "[1,   900] loss: 1.285\n",
            "[2,   100] loss: 1.220\n",
            "[2,   200] loss: 1.209\n",
            "[2,   300] loss: 1.157\n",
            "[2,   400] loss: 1.155\n",
            "[2,   500] loss: 1.130\n",
            "[2,   600] loss: 1.112\n",
            "[2,   700] loss: 1.094\n",
            "[2,   800] loss: 1.090\n",
            "[2,   900] loss: 1.049\n",
            "[3,   100] loss: 1.030\n",
            "[3,   200] loss: 1.047\n",
            "[3,   300] loss: 1.016\n",
            "[3,   400] loss: 1.015\n",
            "[3,   500] loss: 1.027\n",
            "[3,   600] loss: 1.005\n",
            "[3,   700] loss: 0.988\n",
            "[3,   800] loss: 0.984\n",
            "[3,   900] loss: 0.988\n",
            "[4,   100] loss: 0.971\n",
            "[4,   200] loss: 0.974\n",
            "[4,   300] loss: 0.942\n",
            "[4,   400] loss: 0.960\n",
            "[4,   500] loss: 0.963\n",
            "[4,   600] loss: 0.941\n",
            "[4,   700] loss: 0.922\n",
            "[4,   800] loss: 0.942\n",
            "[4,   900] loss: 0.942\n",
            "[5,   100] loss: 0.911\n",
            "[5,   200] loss: 0.915\n",
            "[5,   300] loss: 0.898\n",
            "[5,   400] loss: 0.916\n",
            "[5,   500] loss: 0.909\n",
            "[5,   600] loss: 0.909\n",
            "[5,   700] loss: 0.900\n",
            "[5,   800] loss: 0.913\n",
            "[5,   900] loss: 0.899\n",
            "[6,   100] loss: 0.896\n",
            "[6,   200] loss: 0.868\n",
            "[6,   300] loss: 0.870\n",
            "[6,   400] loss: 0.875\n",
            "[6,   500] loss: 0.903\n",
            "[6,   600] loss: 0.891\n",
            "[6,   700] loss: 0.885\n",
            "[6,   800] loss: 0.868\n",
            "[6,   900] loss: 0.874\n",
            "[7,   100] loss: 0.862\n",
            "[7,   200] loss: 0.885\n",
            "[7,   300] loss: 0.855\n",
            "[7,   400] loss: 0.854\n",
            "[7,   500] loss: 0.847\n",
            "[7,   600] loss: 0.850\n",
            "[7,   700] loss: 0.845\n",
            "[7,   800] loss: 0.847\n",
            "[7,   900] loss: 0.842\n",
            "[8,   100] loss: 0.868\n",
            "[8,   200] loss: 0.833\n",
            "[8,   300] loss: 0.859\n",
            "[8,   400] loss: 0.824\n",
            "[8,   500] loss: 0.847\n",
            "[8,   600] loss: 0.827\n",
            "[8,   700] loss: 0.831\n",
            "[8,   800] loss: 0.834\n",
            "[8,   900] loss: 0.819\n",
            "[9,   100] loss: 0.823\n",
            "[9,   200] loss: 0.801\n",
            "[9,   300] loss: 0.823\n",
            "[9,   400] loss: 0.812\n",
            "[9,   500] loss: 0.822\n",
            "[9,   600] loss: 0.808\n",
            "[9,   700] loss: 0.828\n",
            "[9,   800] loss: 0.801\n",
            "[9,   900] loss: 0.807\n",
            "[10,   100] loss: 0.828\n",
            "[10,   200] loss: 0.787\n",
            "[10,   300] loss: 0.803\n",
            "[10,   400] loss: 0.804\n",
            "[10,   500] loss: 0.787\n",
            "[10,   600] loss: 0.795\n",
            "[10,   700] loss: 0.799\n",
            "[10,   800] loss: 0.789\n",
            "[10,   900] loss: 0.791\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd5cae69880>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It imports the required libraries: torch, torchvision, torch.nn, torch.nn.functional, and torch.optim.\n",
        "It defines a neural network architecture called Net with two fully connected layers: the first layer has 784 input units (since the images are 28x28=784 pixels), 20 hidden units, and a hyperbolic tangent activation function, and the second layer has 10 output units (one for each digit) and no activation function.\n",
        "It defines a set of image transformations using the torchvision.transforms.Compose function: it rotates the images by a random angle between -90 and 90 degrees, converts them to tensors, and normalizes the pixel values to have a mean of 0.5 and standard deviation of 0.5.\n",
        "It loads the MNIST dataset using the torchvision.datasets.MNIST function, specifying the root directory where the dataset will be stored, that it's the training set, that it should be downloaded if it's not already, and that the defined transformations should be applied to it.\n",
        "It creates a DataLoader object that iterates over the dataset in batches of size 64, shuffles the samples in each epoch, and uses two worker processes to load the data in parallel.\n",
        "It initializes the neural network model, defines the loss function as cross-entropy, and the optimizer as stochastic gradient descent with a learning rate of 0.001 and momentum of 0.9.\n",
        "It trains the neural network for 10 epochs using a nested loop: in the outer loop, it iterates over the epochs, and in the inner loop, it iterates over the mini-batches of data. For each mini-batch, it performs the forward pass through the network to compute the logits, computes the loss, performs the backward pass to compute the gradients, and updates the network parameters using the optimizer.\n",
        "It prints the running loss every 100 mini-batches during training.\n",
        "After training, it prints a message indicating that the training has finished.\n",
        "\n"
      ],
      "metadata": {
        "id": "871mMmCeZKed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we first define a neural network with one hidden layer and train it on the entire MNIST dataset for 10 epochs. We then test the accuracy of the network on the test set.\n",
        "\n",
        "Next, we train the same network on a subset of the MNIST dataset (the first 5000 images) for another 10 epochs. We then test the accuracy of the network on the entire MNIST dataset again to see if the network has suffered from catastrophic forgetting.\n",
        "\n",
        "The results show that the network has indeed suffered from catastrophic forgetting, as its accuracy on the entire MNIST dataset after training on the subset is significantly lower than its accuracy when trained on the entire dataset from scratch.\n"
      ],
      "metadata": {
        "id": "E_9R9-ufLDRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the transformations to be applied to the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "full_trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "full_testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into two subsets: old (digits 0-4) and new (digits 5-9)\n",
        "old_trainset = [(x, y) for x, y in full_trainset if y < 5]\n",
        "old_testset = [(x, y) for x, y in full_testset if y < 5]\n",
        "new_trainset = [(x, y) for x, y in full_trainset if y >= 5]\n",
        "new_testset = [(x, y) for x, y in full_testset if y >= 5]\n",
        "\n",
        "# Define data loaders for the subsets\n",
        "old_train_loader = torch.utils.data.DataLoader(old_trainset, batch_size=64, shuffle=True)\n",
        "old_test_loader = torch.utils.data.DataLoader(old_testset, batch_size=64, shuffle=False)\n",
        "new_train_loader = torch.utils.data.DataLoader(new_trainset, batch_size=64, shuffle=True)\n",
        "new_test_loader = torch.utils.data.DataLoader(new_testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv1(x), 2))\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# Define the training function\n",
        "def train(model, dataloader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        if target.max() >= 5:\n",
        "            target = 9 - target\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
        "                100. * batch_idx / len(dataloader), loss.item()))\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            output = model(data)\n",
        "            if target.max() >= 5:\n",
        "                _, predicted = torch.min(output.data, 1)\n",
        "            else:\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = 100. * correct / len(dataloader.dataset)\n",
        "    print('Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        correct, len(dataloader.dataset), accuracy))\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model on the old subset\n",
        "old_accuracies = []\n",
        "for epoch in range(1, 6):\n",
        "    train(model, old_train_loader, optimizer, criterion, epoch)\n",
        "    accuracy = test(model, old_test_loader)\n",
        "    old_accuracies.append(accuracy)\n",
        "\n",
        "# Test the model on the old and new subsets and print the accuracies\n",
        "old_accuracy = test(model, old_test_loader)\n",
        "#new_accuracy = test(model, new_test_loader)\n",
        "print('Accuracy on old subset when trained on old subset:', old_accuracy)\n",
        "#print('Accuracy on new subset when trained on old subset:', new_accuracy)\n",
        "\n",
        "# Train the model on the new subset\n",
        "new_accuracies = []\n",
        "for epoch in range(1, 6):\n",
        "    train(model, new_train_loader, optimizer, criterion, epoch)\n",
        "    accuracy = test(model, new_test_loader)\n",
        "    new_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "# Test the model on the old and new subsets after training on the new subset and print the accuracies\n",
        "old_accuracy_after = test(model, old_test_loader)\n",
        "#new_accuracy_after = test(model, new_test_loader)\n",
        "print('Accuracy on old subset after training on new subset:', old_accuracy_after)\n",
        "#print('Accuracy on new subset after training on new subset:', new_accuracy_after)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVgbGqcLFfR",
        "outputId": "af0f3f48-2f99-4408-d3c9-bc2c49d0945e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/30596 (0%)]\tLoss: 1.613122\n",
            "Train Epoch: 1 [640/30596 (2%)]\tLoss: 1.609941\n",
            "Train Epoch: 1 [1280/30596 (4%)]\tLoss: 1.611119\n",
            "Train Epoch: 1 [1920/30596 (6%)]\tLoss: 1.597775\n",
            "Train Epoch: 1 [2560/30596 (8%)]\tLoss: 1.582379\n",
            "Train Epoch: 1 [3200/30596 (10%)]\tLoss: 1.572191\n",
            "Train Epoch: 1 [3840/30596 (13%)]\tLoss: 1.579628\n",
            "Train Epoch: 1 [4480/30596 (15%)]\tLoss: 1.573015\n",
            "Train Epoch: 1 [5120/30596 (17%)]\tLoss: 1.566074\n",
            "Train Epoch: 1 [5760/30596 (19%)]\tLoss: 1.543425\n",
            "Train Epoch: 1 [6400/30596 (21%)]\tLoss: 1.561422\n",
            "Train Epoch: 1 [7040/30596 (23%)]\tLoss: 1.535854\n",
            "Train Epoch: 1 [7680/30596 (25%)]\tLoss: 1.521164\n",
            "Train Epoch: 1 [8320/30596 (27%)]\tLoss: 1.532882\n",
            "Train Epoch: 1 [8960/30596 (29%)]\tLoss: 1.516374\n",
            "Train Epoch: 1 [9600/30596 (31%)]\tLoss: 1.486310\n",
            "Train Epoch: 1 [10240/30596 (33%)]\tLoss: 1.465419\n",
            "Train Epoch: 1 [10880/30596 (35%)]\tLoss: 1.409983\n",
            "Train Epoch: 1 [11520/30596 (38%)]\tLoss: 1.407446\n",
            "Train Epoch: 1 [12160/30596 (40%)]\tLoss: 1.417463\n",
            "Train Epoch: 1 [12800/30596 (42%)]\tLoss: 1.276139\n",
            "Train Epoch: 1 [13440/30596 (44%)]\tLoss: 1.304816\n",
            "Train Epoch: 1 [14080/30596 (46%)]\tLoss: 1.243056\n",
            "Train Epoch: 1 [14720/30596 (48%)]\tLoss: 1.201181\n",
            "Train Epoch: 1 [15360/30596 (50%)]\tLoss: 1.195994\n",
            "Train Epoch: 1 [16000/30596 (52%)]\tLoss: 1.123772\n",
            "Train Epoch: 1 [16640/30596 (54%)]\tLoss: 1.077843\n",
            "Train Epoch: 1 [17280/30596 (56%)]\tLoss: 0.973897\n",
            "Train Epoch: 1 [17920/30596 (58%)]\tLoss: 1.076715\n",
            "Train Epoch: 1 [18560/30596 (61%)]\tLoss: 0.844140\n",
            "Train Epoch: 1 [19200/30596 (63%)]\tLoss: 0.856113\n",
            "Train Epoch: 1 [19840/30596 (65%)]\tLoss: 0.792207\n",
            "Train Epoch: 1 [20480/30596 (67%)]\tLoss: 0.825858\n",
            "Train Epoch: 1 [21120/30596 (69%)]\tLoss: 0.808949\n",
            "Train Epoch: 1 [21760/30596 (71%)]\tLoss: 0.808940\n",
            "Train Epoch: 1 [22400/30596 (73%)]\tLoss: 0.830063\n",
            "Train Epoch: 1 [23040/30596 (75%)]\tLoss: 0.622658\n",
            "Train Epoch: 1 [23680/30596 (77%)]\tLoss: 0.632462\n",
            "Train Epoch: 1 [24320/30596 (79%)]\tLoss: 0.622384\n",
            "Train Epoch: 1 [24960/30596 (81%)]\tLoss: 0.754961\n",
            "Train Epoch: 1 [25600/30596 (84%)]\tLoss: 0.615042\n",
            "Train Epoch: 1 [26240/30596 (86%)]\tLoss: 0.595098\n",
            "Train Epoch: 1 [26880/30596 (88%)]\tLoss: 0.548523\n",
            "Train Epoch: 1 [27520/30596 (90%)]\tLoss: 0.642634\n",
            "Train Epoch: 1 [28160/30596 (92%)]\tLoss: 0.534406\n",
            "Train Epoch: 1 [28800/30596 (94%)]\tLoss: 0.569146\n",
            "Train Epoch: 1 [29440/30596 (96%)]\tLoss: 0.432968\n",
            "Train Epoch: 1 [30080/30596 (98%)]\tLoss: 0.547547\n",
            "Accuracy: 4171/5139 (81.16%)\n",
            "Train Epoch: 2 [0/30596 (0%)]\tLoss: 0.603184\n",
            "Train Epoch: 2 [640/30596 (2%)]\tLoss: 0.630583\n",
            "Train Epoch: 2 [1280/30596 (4%)]\tLoss: 0.580879\n",
            "Train Epoch: 2 [1920/30596 (6%)]\tLoss: 0.515255\n",
            "Train Epoch: 2 [2560/30596 (8%)]\tLoss: 0.464346\n",
            "Train Epoch: 2 [3200/30596 (10%)]\tLoss: 0.384548\n",
            "Train Epoch: 2 [3840/30596 (13%)]\tLoss: 0.549088\n",
            "Train Epoch: 2 [4480/30596 (15%)]\tLoss: 0.350581\n",
            "Train Epoch: 2 [5120/30596 (17%)]\tLoss: 0.484771\n",
            "Train Epoch: 2 [5760/30596 (19%)]\tLoss: 0.355714\n",
            "Train Epoch: 2 [6400/30596 (21%)]\tLoss: 0.377545\n",
            "Train Epoch: 2 [7040/30596 (23%)]\tLoss: 0.475013\n",
            "Train Epoch: 2 [7680/30596 (25%)]\tLoss: 0.403049\n",
            "Train Epoch: 2 [8320/30596 (27%)]\tLoss: 0.406855\n",
            "Train Epoch: 2 [8960/30596 (29%)]\tLoss: 0.436653\n",
            "Train Epoch: 2 [9600/30596 (31%)]\tLoss: 0.340065\n",
            "Train Epoch: 2 [10240/30596 (33%)]\tLoss: 0.418260\n",
            "Train Epoch: 2 [10880/30596 (35%)]\tLoss: 0.432788\n",
            "Train Epoch: 2 [11520/30596 (38%)]\tLoss: 0.480314\n",
            "Train Epoch: 2 [12160/30596 (40%)]\tLoss: 0.441549\n",
            "Train Epoch: 2 [12800/30596 (42%)]\tLoss: 0.321800\n",
            "Train Epoch: 2 [13440/30596 (44%)]\tLoss: 0.273485\n",
            "Train Epoch: 2 [14080/30596 (46%)]\tLoss: 0.414034\n",
            "Train Epoch: 2 [14720/30596 (48%)]\tLoss: 0.342840\n",
            "Train Epoch: 2 [15360/30596 (50%)]\tLoss: 0.321291\n",
            "Train Epoch: 2 [16000/30596 (52%)]\tLoss: 0.309073\n",
            "Train Epoch: 2 [16640/30596 (54%)]\tLoss: 0.376943\n",
            "Train Epoch: 2 [17280/30596 (56%)]\tLoss: 0.340623\n",
            "Train Epoch: 2 [17920/30596 (58%)]\tLoss: 0.391315\n",
            "Train Epoch: 2 [18560/30596 (61%)]\tLoss: 0.301620\n",
            "Train Epoch: 2 [19200/30596 (63%)]\tLoss: 0.438077\n",
            "Train Epoch: 2 [19840/30596 (65%)]\tLoss: 0.447727\n",
            "Train Epoch: 2 [20480/30596 (67%)]\tLoss: 0.304029\n",
            "Train Epoch: 2 [21120/30596 (69%)]\tLoss: 0.316041\n",
            "Train Epoch: 2 [21760/30596 (71%)]\tLoss: 0.261919\n",
            "Train Epoch: 2 [22400/30596 (73%)]\tLoss: 0.264823\n",
            "Train Epoch: 2 [23040/30596 (75%)]\tLoss: 0.463151\n",
            "Train Epoch: 2 [23680/30596 (77%)]\tLoss: 0.340425\n",
            "Train Epoch: 2 [24320/30596 (79%)]\tLoss: 0.287735\n",
            "Train Epoch: 2 [24960/30596 (81%)]\tLoss: 0.306347\n",
            "Train Epoch: 2 [25600/30596 (84%)]\tLoss: 0.257608\n",
            "Train Epoch: 2 [26240/30596 (86%)]\tLoss: 0.330502\n",
            "Train Epoch: 2 [26880/30596 (88%)]\tLoss: 0.372574\n",
            "Train Epoch: 2 [27520/30596 (90%)]\tLoss: 0.247808\n",
            "Train Epoch: 2 [28160/30596 (92%)]\tLoss: 0.263125\n",
            "Train Epoch: 2 [28800/30596 (94%)]\tLoss: 0.245801\n",
            "Train Epoch: 2 [29440/30596 (96%)]\tLoss: 0.297859\n",
            "Train Epoch: 2 [30080/30596 (98%)]\tLoss: 0.399886\n",
            "Accuracy: 4684/5139 (91.15%)\n",
            "Train Epoch: 3 [0/30596 (0%)]\tLoss: 0.279975\n",
            "Train Epoch: 3 [640/30596 (2%)]\tLoss: 0.349979\n",
            "Train Epoch: 3 [1280/30596 (4%)]\tLoss: 0.511573\n",
            "Train Epoch: 3 [1920/30596 (6%)]\tLoss: 0.348591\n",
            "Train Epoch: 3 [2560/30596 (8%)]\tLoss: 0.333925\n",
            "Train Epoch: 3 [3200/30596 (10%)]\tLoss: 0.292186\n",
            "Train Epoch: 3 [3840/30596 (13%)]\tLoss: 0.347132\n",
            "Train Epoch: 3 [4480/30596 (15%)]\tLoss: 0.431736\n",
            "Train Epoch: 3 [5120/30596 (17%)]\tLoss: 0.280769\n",
            "Train Epoch: 3 [5760/30596 (19%)]\tLoss: 0.302555\n",
            "Train Epoch: 3 [6400/30596 (21%)]\tLoss: 0.341183\n",
            "Train Epoch: 3 [7040/30596 (23%)]\tLoss: 0.310635\n",
            "Train Epoch: 3 [7680/30596 (25%)]\tLoss: 0.290633\n",
            "Train Epoch: 3 [8320/30596 (27%)]\tLoss: 0.164227\n",
            "Train Epoch: 3 [8960/30596 (29%)]\tLoss: 0.362543\n",
            "Train Epoch: 3 [9600/30596 (31%)]\tLoss: 0.361735\n",
            "Train Epoch: 3 [10240/30596 (33%)]\tLoss: 0.217499\n",
            "Train Epoch: 3 [10880/30596 (35%)]\tLoss: 0.163510\n",
            "Train Epoch: 3 [11520/30596 (38%)]\tLoss: 0.258432\n",
            "Train Epoch: 3 [12160/30596 (40%)]\tLoss: 0.117677\n",
            "Train Epoch: 3 [12800/30596 (42%)]\tLoss: 0.267500\n",
            "Train Epoch: 3 [13440/30596 (44%)]\tLoss: 0.270558\n",
            "Train Epoch: 3 [14080/30596 (46%)]\tLoss: 0.293950\n",
            "Train Epoch: 3 [14720/30596 (48%)]\tLoss: 0.245968\n",
            "Train Epoch: 3 [15360/30596 (50%)]\tLoss: 0.243591\n",
            "Train Epoch: 3 [16000/30596 (52%)]\tLoss: 0.212424\n",
            "Train Epoch: 3 [16640/30596 (54%)]\tLoss: 0.242756\n",
            "Train Epoch: 3 [17280/30596 (56%)]\tLoss: 0.256002\n",
            "Train Epoch: 3 [17920/30596 (58%)]\tLoss: 0.199397\n",
            "Train Epoch: 3 [18560/30596 (61%)]\tLoss: 0.230628\n",
            "Train Epoch: 3 [19200/30596 (63%)]\tLoss: 0.149235\n",
            "Train Epoch: 3 [19840/30596 (65%)]\tLoss: 0.241101\n",
            "Train Epoch: 3 [20480/30596 (67%)]\tLoss: 0.142103\n",
            "Train Epoch: 3 [21120/30596 (69%)]\tLoss: 0.277893\n",
            "Train Epoch: 3 [21760/30596 (71%)]\tLoss: 0.405734\n",
            "Train Epoch: 3 [22400/30596 (73%)]\tLoss: 0.121728\n",
            "Train Epoch: 3 [23040/30596 (75%)]\tLoss: 0.220444\n",
            "Train Epoch: 3 [23680/30596 (77%)]\tLoss: 0.271627\n",
            "Train Epoch: 3 [24320/30596 (79%)]\tLoss: 0.191977\n",
            "Train Epoch: 3 [24960/30596 (81%)]\tLoss: 0.224450\n",
            "Train Epoch: 3 [25600/30596 (84%)]\tLoss: 0.355168\n",
            "Train Epoch: 3 [26240/30596 (86%)]\tLoss: 0.221443\n",
            "Train Epoch: 3 [26880/30596 (88%)]\tLoss: 0.187204\n",
            "Train Epoch: 3 [27520/30596 (90%)]\tLoss: 0.399804\n",
            "Train Epoch: 3 [28160/30596 (92%)]\tLoss: 0.199872\n",
            "Train Epoch: 3 [28800/30596 (94%)]\tLoss: 0.255468\n",
            "Train Epoch: 3 [29440/30596 (96%)]\tLoss: 0.190169\n",
            "Train Epoch: 3 [30080/30596 (98%)]\tLoss: 0.332099\n",
            "Accuracy: 4826/5139 (93.91%)\n",
            "Train Epoch: 4 [0/30596 (0%)]\tLoss: 0.177316\n",
            "Train Epoch: 4 [640/30596 (2%)]\tLoss: 0.216326\n",
            "Train Epoch: 4 [1280/30596 (4%)]\tLoss: 0.249283\n",
            "Train Epoch: 4 [1920/30596 (6%)]\tLoss: 0.219835\n",
            "Train Epoch: 4 [2560/30596 (8%)]\tLoss: 0.271210\n",
            "Train Epoch: 4 [3200/30596 (10%)]\tLoss: 0.252155\n",
            "Train Epoch: 4 [3840/30596 (13%)]\tLoss: 0.245074\n",
            "Train Epoch: 4 [4480/30596 (15%)]\tLoss: 0.107491\n",
            "Train Epoch: 4 [5120/30596 (17%)]\tLoss: 0.392279\n",
            "Train Epoch: 4 [5760/30596 (19%)]\tLoss: 0.193800\n",
            "Train Epoch: 4 [6400/30596 (21%)]\tLoss: 0.054754\n",
            "Train Epoch: 4 [7040/30596 (23%)]\tLoss: 0.195411\n",
            "Train Epoch: 4 [7680/30596 (25%)]\tLoss: 0.161281\n",
            "Train Epoch: 4 [8320/30596 (27%)]\tLoss: 0.104570\n",
            "Train Epoch: 4 [8960/30596 (29%)]\tLoss: 0.217739\n",
            "Train Epoch: 4 [9600/30596 (31%)]\tLoss: 0.259224\n",
            "Train Epoch: 4 [10240/30596 (33%)]\tLoss: 0.202698\n",
            "Train Epoch: 4 [10880/30596 (35%)]\tLoss: 0.137260\n",
            "Train Epoch: 4 [11520/30596 (38%)]\tLoss: 0.165077\n",
            "Train Epoch: 4 [12160/30596 (40%)]\tLoss: 0.224937\n",
            "Train Epoch: 4 [12800/30596 (42%)]\tLoss: 0.135779\n",
            "Train Epoch: 4 [13440/30596 (44%)]\tLoss: 0.346966\n",
            "Train Epoch: 4 [14080/30596 (46%)]\tLoss: 0.192001\n",
            "Train Epoch: 4 [14720/30596 (48%)]\tLoss: 0.156633\n",
            "Train Epoch: 4 [15360/30596 (50%)]\tLoss: 0.459328\n",
            "Train Epoch: 4 [16000/30596 (52%)]\tLoss: 0.166674\n",
            "Train Epoch: 4 [16640/30596 (54%)]\tLoss: 0.116488\n",
            "Train Epoch: 4 [17280/30596 (56%)]\tLoss: 0.168021\n",
            "Train Epoch: 4 [17920/30596 (58%)]\tLoss: 0.197426\n",
            "Train Epoch: 4 [18560/30596 (61%)]\tLoss: 0.192468\n",
            "Train Epoch: 4 [19200/30596 (63%)]\tLoss: 0.232105\n",
            "Train Epoch: 4 [19840/30596 (65%)]\tLoss: 0.237467\n",
            "Train Epoch: 4 [20480/30596 (67%)]\tLoss: 0.120829\n",
            "Train Epoch: 4 [21120/30596 (69%)]\tLoss: 0.215428\n",
            "Train Epoch: 4 [21760/30596 (71%)]\tLoss: 0.179204\n",
            "Train Epoch: 4 [22400/30596 (73%)]\tLoss: 0.361107\n",
            "Train Epoch: 4 [23040/30596 (75%)]\tLoss: 0.226477\n",
            "Train Epoch: 4 [23680/30596 (77%)]\tLoss: 0.251722\n",
            "Train Epoch: 4 [24320/30596 (79%)]\tLoss: 0.235462\n",
            "Train Epoch: 4 [24960/30596 (81%)]\tLoss: 0.101317\n",
            "Train Epoch: 4 [25600/30596 (84%)]\tLoss: 0.091113\n",
            "Train Epoch: 4 [26240/30596 (86%)]\tLoss: 0.092186\n",
            "Train Epoch: 4 [26880/30596 (88%)]\tLoss: 0.393734\n",
            "Train Epoch: 4 [27520/30596 (90%)]\tLoss: 0.164475\n",
            "Train Epoch: 4 [28160/30596 (92%)]\tLoss: 0.071337\n",
            "Train Epoch: 4 [28800/30596 (94%)]\tLoss: 0.292131\n",
            "Train Epoch: 4 [29440/30596 (96%)]\tLoss: 0.087407\n",
            "Train Epoch: 4 [30080/30596 (98%)]\tLoss: 0.162828\n",
            "Accuracy: 4712/5139 (91.69%)\n",
            "Train Epoch: 5 [0/30596 (0%)]\tLoss: 0.214287\n",
            "Train Epoch: 5 [640/30596 (2%)]\tLoss: 0.284776\n",
            "Train Epoch: 5 [1280/30596 (4%)]\tLoss: 0.088670\n",
            "Train Epoch: 5 [1920/30596 (6%)]\tLoss: 0.116897\n",
            "Train Epoch: 5 [2560/30596 (8%)]\tLoss: 0.154953\n",
            "Train Epoch: 5 [3200/30596 (10%)]\tLoss: 0.083607\n",
            "Train Epoch: 5 [3840/30596 (13%)]\tLoss: 0.354527\n",
            "Train Epoch: 5 [4480/30596 (15%)]\tLoss: 0.179468\n",
            "Train Epoch: 5 [5120/30596 (17%)]\tLoss: 0.196547\n",
            "Train Epoch: 5 [5760/30596 (19%)]\tLoss: 0.226865\n",
            "Train Epoch: 5 [6400/30596 (21%)]\tLoss: 0.258822\n",
            "Train Epoch: 5 [7040/30596 (23%)]\tLoss: 0.178451\n",
            "Train Epoch: 5 [7680/30596 (25%)]\tLoss: 0.330962\n",
            "Train Epoch: 5 [8320/30596 (27%)]\tLoss: 0.361011\n",
            "Train Epoch: 5 [8960/30596 (29%)]\tLoss: 0.079691\n",
            "Train Epoch: 5 [9600/30596 (31%)]\tLoss: 0.111976\n",
            "Train Epoch: 5 [10240/30596 (33%)]\tLoss: 0.094229\n",
            "Train Epoch: 5 [10880/30596 (35%)]\tLoss: 0.084906\n",
            "Train Epoch: 5 [11520/30596 (38%)]\tLoss: 0.172057\n",
            "Train Epoch: 5 [12160/30596 (40%)]\tLoss: 0.067512\n",
            "Train Epoch: 5 [12800/30596 (42%)]\tLoss: 0.068352\n",
            "Train Epoch: 5 [13440/30596 (44%)]\tLoss: 0.167221\n",
            "Train Epoch: 5 [14080/30596 (46%)]\tLoss: 0.247429\n",
            "Train Epoch: 5 [14720/30596 (48%)]\tLoss: 0.280196\n",
            "Train Epoch: 5 [15360/30596 (50%)]\tLoss: 0.183111\n",
            "Train Epoch: 5 [16000/30596 (52%)]\tLoss: 0.262428\n",
            "Train Epoch: 5 [16640/30596 (54%)]\tLoss: 0.205313\n",
            "Train Epoch: 5 [17280/30596 (56%)]\tLoss: 0.244075\n",
            "Train Epoch: 5 [17920/30596 (58%)]\tLoss: 0.236067\n",
            "Train Epoch: 5 [18560/30596 (61%)]\tLoss: 0.102484\n",
            "Train Epoch: 5 [19200/30596 (63%)]\tLoss: 0.083703\n",
            "Train Epoch: 5 [19840/30596 (65%)]\tLoss: 0.061590\n",
            "Train Epoch: 5 [20480/30596 (67%)]\tLoss: 0.170524\n",
            "Train Epoch: 5 [21120/30596 (69%)]\tLoss: 0.090583\n",
            "Train Epoch: 5 [21760/30596 (71%)]\tLoss: 0.222769\n",
            "Train Epoch: 5 [22400/30596 (73%)]\tLoss: 0.071362\n",
            "Train Epoch: 5 [23040/30596 (75%)]\tLoss: 0.281726\n",
            "Train Epoch: 5 [23680/30596 (77%)]\tLoss: 0.193563\n",
            "Train Epoch: 5 [24320/30596 (79%)]\tLoss: 0.273027\n",
            "Train Epoch: 5 [24960/30596 (81%)]\tLoss: 0.191295\n",
            "Train Epoch: 5 [25600/30596 (84%)]\tLoss: 0.273020\n",
            "Train Epoch: 5 [26240/30596 (86%)]\tLoss: 0.138584\n",
            "Train Epoch: 5 [26880/30596 (88%)]\tLoss: 0.171369\n",
            "Train Epoch: 5 [27520/30596 (90%)]\tLoss: 0.262172\n",
            "Train Epoch: 5 [28160/30596 (92%)]\tLoss: 0.164560\n",
            "Train Epoch: 5 [28800/30596 (94%)]\tLoss: 0.150934\n",
            "Train Epoch: 5 [29440/30596 (96%)]\tLoss: 0.197357\n",
            "Train Epoch: 5 [30080/30596 (98%)]\tLoss: 0.152081\n",
            "Accuracy: 4942/5139 (96.17%)\n",
            "Accuracy: 4942/5139 (96.17%)\n",
            "Accuracy on old subset when trained on old subset: 96.16656937147305\n",
            "Train Epoch: 1 [0/29404 (0%)]\tLoss: 4.362508\n",
            "Train Epoch: 1 [640/29404 (2%)]\tLoss: 1.329563\n",
            "Train Epoch: 1 [1280/29404 (4%)]\tLoss: 0.859903\n",
            "Train Epoch: 1 [1920/29404 (7%)]\tLoss: 0.922957\n",
            "Train Epoch: 1 [2560/29404 (9%)]\tLoss: 0.739900\n",
            "Train Epoch: 1 [3200/29404 (11%)]\tLoss: 0.683106\n",
            "Train Epoch: 1 [3840/29404 (13%)]\tLoss: 0.706683\n",
            "Train Epoch: 1 [4480/29404 (15%)]\tLoss: 0.549987\n",
            "Train Epoch: 1 [5120/29404 (17%)]\tLoss: 0.730549\n",
            "Train Epoch: 1 [5760/29404 (20%)]\tLoss: 0.423860\n",
            "Train Epoch: 1 [6400/29404 (22%)]\tLoss: 0.464915\n",
            "Train Epoch: 1 [7040/29404 (24%)]\tLoss: 0.575383\n",
            "Train Epoch: 1 [7680/29404 (26%)]\tLoss: 0.415478\n",
            "Train Epoch: 1 [8320/29404 (28%)]\tLoss: 0.426815\n",
            "Train Epoch: 1 [8960/29404 (30%)]\tLoss: 0.502196\n",
            "Train Epoch: 1 [9600/29404 (33%)]\tLoss: 0.579865\n",
            "Train Epoch: 1 [10240/29404 (35%)]\tLoss: 0.586486\n",
            "Train Epoch: 1 [10880/29404 (37%)]\tLoss: 0.384321\n",
            "Train Epoch: 1 [11520/29404 (39%)]\tLoss: 0.508482\n",
            "Train Epoch: 1 [12160/29404 (41%)]\tLoss: 0.477029\n",
            "Train Epoch: 1 [12800/29404 (43%)]\tLoss: 0.436114\n",
            "Train Epoch: 1 [13440/29404 (46%)]\tLoss: 0.726308\n",
            "Train Epoch: 1 [14080/29404 (48%)]\tLoss: 0.344332\n",
            "Train Epoch: 1 [14720/29404 (50%)]\tLoss: 0.416034\n",
            "Train Epoch: 1 [15360/29404 (52%)]\tLoss: 0.357132\n",
            "Train Epoch: 1 [16000/29404 (54%)]\tLoss: 0.411764\n",
            "Train Epoch: 1 [16640/29404 (57%)]\tLoss: 0.307859\n",
            "Train Epoch: 1 [17280/29404 (59%)]\tLoss: 0.438466\n",
            "Train Epoch: 1 [17920/29404 (61%)]\tLoss: 0.495250\n",
            "Train Epoch: 1 [18560/29404 (63%)]\tLoss: 0.424748\n",
            "Train Epoch: 1 [19200/29404 (65%)]\tLoss: 0.424535\n",
            "Train Epoch: 1 [19840/29404 (67%)]\tLoss: 0.409930\n",
            "Train Epoch: 1 [20480/29404 (70%)]\tLoss: 0.314557\n",
            "Train Epoch: 1 [21120/29404 (72%)]\tLoss: 0.347623\n",
            "Train Epoch: 1 [21760/29404 (74%)]\tLoss: 0.330940\n",
            "Train Epoch: 1 [22400/29404 (76%)]\tLoss: 0.423358\n",
            "Train Epoch: 1 [23040/29404 (78%)]\tLoss: 0.519665\n",
            "Train Epoch: 1 [23680/29404 (80%)]\tLoss: 0.301690\n",
            "Train Epoch: 1 [24320/29404 (83%)]\tLoss: 0.362764\n",
            "Train Epoch: 1 [24960/29404 (85%)]\tLoss: 0.341224\n",
            "Train Epoch: 1 [25600/29404 (87%)]\tLoss: 0.514618\n",
            "Train Epoch: 1 [26240/29404 (89%)]\tLoss: 0.266791\n",
            "Train Epoch: 1 [26880/29404 (91%)]\tLoss: 0.268911\n",
            "Train Epoch: 1 [27520/29404 (93%)]\tLoss: 0.268004\n",
            "Train Epoch: 1 [28160/29404 (96%)]\tLoss: 0.441075\n",
            "Train Epoch: 1 [28800/29404 (98%)]\tLoss: 0.221059\n",
            "Accuracy: 0/4861 (0.00%)\n",
            "Train Epoch: 2 [0/29404 (0%)]\tLoss: 0.248208\n",
            "Train Epoch: 2 [640/29404 (2%)]\tLoss: 0.380325\n",
            "Train Epoch: 2 [1280/29404 (4%)]\tLoss: 0.387456\n",
            "Train Epoch: 2 [1920/29404 (7%)]\tLoss: 0.419738\n",
            "Train Epoch: 2 [2560/29404 (9%)]\tLoss: 0.219526\n",
            "Train Epoch: 2 [3200/29404 (11%)]\tLoss: 0.350645\n",
            "Train Epoch: 2 [3840/29404 (13%)]\tLoss: 0.242066\n",
            "Train Epoch: 2 [4480/29404 (15%)]\tLoss: 0.233487\n",
            "Train Epoch: 2 [5120/29404 (17%)]\tLoss: 0.294916\n",
            "Train Epoch: 2 [5760/29404 (20%)]\tLoss: 0.254051\n",
            "Train Epoch: 2 [6400/29404 (22%)]\tLoss: 0.238347\n",
            "Train Epoch: 2 [7040/29404 (24%)]\tLoss: 0.313879\n",
            "Train Epoch: 2 [7680/29404 (26%)]\tLoss: 0.445033\n",
            "Train Epoch: 2 [8320/29404 (28%)]\tLoss: 0.325421\n",
            "Train Epoch: 2 [8960/29404 (30%)]\tLoss: 0.267645\n",
            "Train Epoch: 2 [9600/29404 (33%)]\tLoss: 0.335659\n",
            "Train Epoch: 2 [10240/29404 (35%)]\tLoss: 0.299930\n",
            "Train Epoch: 2 [10880/29404 (37%)]\tLoss: 0.454416\n",
            "Train Epoch: 2 [11520/29404 (39%)]\tLoss: 0.216226\n",
            "Train Epoch: 2 [12160/29404 (41%)]\tLoss: 0.306002\n",
            "Train Epoch: 2 [12800/29404 (43%)]\tLoss: 0.346147\n",
            "Train Epoch: 2 [13440/29404 (46%)]\tLoss: 0.433590\n",
            "Train Epoch: 2 [14080/29404 (48%)]\tLoss: 0.304256\n",
            "Train Epoch: 2 [14720/29404 (50%)]\tLoss: 0.152465\n",
            "Train Epoch: 2 [15360/29404 (52%)]\tLoss: 0.278629\n",
            "Train Epoch: 2 [16000/29404 (54%)]\tLoss: 0.451212\n",
            "Train Epoch: 2 [16640/29404 (57%)]\tLoss: 0.288342\n",
            "Train Epoch: 2 [17280/29404 (59%)]\tLoss: 0.218093\n",
            "Train Epoch: 2 [17920/29404 (61%)]\tLoss: 0.500958\n",
            "Train Epoch: 2 [18560/29404 (63%)]\tLoss: 0.325981\n",
            "Train Epoch: 2 [19200/29404 (65%)]\tLoss: 0.209007\n",
            "Train Epoch: 2 [19840/29404 (67%)]\tLoss: 0.297458\n",
            "Train Epoch: 2 [20480/29404 (70%)]\tLoss: 0.285257\n",
            "Train Epoch: 2 [21120/29404 (72%)]\tLoss: 0.277428\n",
            "Train Epoch: 2 [21760/29404 (74%)]\tLoss: 0.309835\n",
            "Train Epoch: 2 [22400/29404 (76%)]\tLoss: 0.476264\n",
            "Train Epoch: 2 [23040/29404 (78%)]\tLoss: 0.151623\n",
            "Train Epoch: 2 [23680/29404 (80%)]\tLoss: 0.292428\n",
            "Train Epoch: 2 [24320/29404 (83%)]\tLoss: 0.217701\n",
            "Train Epoch: 2 [24960/29404 (85%)]\tLoss: 0.104933\n",
            "Train Epoch: 2 [25600/29404 (87%)]\tLoss: 0.246092\n",
            "Train Epoch: 2 [26240/29404 (89%)]\tLoss: 0.150440\n",
            "Train Epoch: 2 [26880/29404 (91%)]\tLoss: 0.270454\n",
            "Train Epoch: 2 [27520/29404 (93%)]\tLoss: 0.432571\n",
            "Train Epoch: 2 [28160/29404 (96%)]\tLoss: 0.226346\n",
            "Train Epoch: 2 [28800/29404 (98%)]\tLoss: 0.240618\n",
            "Accuracy: 0/4861 (0.00%)\n",
            "Train Epoch: 3 [0/29404 (0%)]\tLoss: 0.166833\n",
            "Train Epoch: 3 [640/29404 (2%)]\tLoss: 0.440313\n",
            "Train Epoch: 3 [1280/29404 (4%)]\tLoss: 0.406640\n",
            "Train Epoch: 3 [1920/29404 (7%)]\tLoss: 0.210158\n",
            "Train Epoch: 3 [2560/29404 (9%)]\tLoss: 0.229366\n",
            "Train Epoch: 3 [3200/29404 (11%)]\tLoss: 0.203191\n",
            "Train Epoch: 3 [3840/29404 (13%)]\tLoss: 0.534831\n",
            "Train Epoch: 3 [4480/29404 (15%)]\tLoss: 0.165531\n",
            "Train Epoch: 3 [5120/29404 (17%)]\tLoss: 0.236553\n",
            "Train Epoch: 3 [5760/29404 (20%)]\tLoss: 0.124961\n",
            "Train Epoch: 3 [6400/29404 (22%)]\tLoss: 0.260588\n",
            "Train Epoch: 3 [7040/29404 (24%)]\tLoss: 0.269698\n",
            "Train Epoch: 3 [7680/29404 (26%)]\tLoss: 0.195491\n",
            "Train Epoch: 3 [8320/29404 (28%)]\tLoss: 0.224859\n",
            "Train Epoch: 3 [8960/29404 (30%)]\tLoss: 0.266049\n",
            "Train Epoch: 3 [9600/29404 (33%)]\tLoss: 0.307117\n",
            "Train Epoch: 3 [10240/29404 (35%)]\tLoss: 0.367166\n",
            "Train Epoch: 3 [10880/29404 (37%)]\tLoss: 0.138004\n",
            "Train Epoch: 3 [11520/29404 (39%)]\tLoss: 0.163536\n",
            "Train Epoch: 3 [12160/29404 (41%)]\tLoss: 0.264145\n",
            "Train Epoch: 3 [12800/29404 (43%)]\tLoss: 0.171332\n",
            "Train Epoch: 3 [13440/29404 (46%)]\tLoss: 0.332649\n",
            "Train Epoch: 3 [14080/29404 (48%)]\tLoss: 0.266961\n",
            "Train Epoch: 3 [14720/29404 (50%)]\tLoss: 0.342692\n",
            "Train Epoch: 3 [15360/29404 (52%)]\tLoss: 0.156100\n",
            "Train Epoch: 3 [16000/29404 (54%)]\tLoss: 0.258291\n",
            "Train Epoch: 3 [16640/29404 (57%)]\tLoss: 0.248022\n",
            "Train Epoch: 3 [17280/29404 (59%)]\tLoss: 0.218883\n",
            "Train Epoch: 3 [17920/29404 (61%)]\tLoss: 0.148400\n",
            "Train Epoch: 3 [18560/29404 (63%)]\tLoss: 0.140411\n",
            "Train Epoch: 3 [19200/29404 (65%)]\tLoss: 0.167491\n",
            "Train Epoch: 3 [19840/29404 (67%)]\tLoss: 0.409807\n",
            "Train Epoch: 3 [20480/29404 (70%)]\tLoss: 0.267389\n",
            "Train Epoch: 3 [21120/29404 (72%)]\tLoss: 0.126134\n",
            "Train Epoch: 3 [21760/29404 (74%)]\tLoss: 0.245164\n",
            "Train Epoch: 3 [22400/29404 (76%)]\tLoss: 0.200053\n",
            "Train Epoch: 3 [23040/29404 (78%)]\tLoss: 0.190906\n",
            "Train Epoch: 3 [23680/29404 (80%)]\tLoss: 0.187911\n",
            "Train Epoch: 3 [24320/29404 (83%)]\tLoss: 0.420544\n",
            "Train Epoch: 3 [24960/29404 (85%)]\tLoss: 0.257209\n",
            "Train Epoch: 3 [25600/29404 (87%)]\tLoss: 0.080087\n",
            "Train Epoch: 3 [26240/29404 (89%)]\tLoss: 0.305990\n",
            "Train Epoch: 3 [26880/29404 (91%)]\tLoss: 0.249601\n",
            "Train Epoch: 3 [27520/29404 (93%)]\tLoss: 0.188163\n",
            "Train Epoch: 3 [28160/29404 (96%)]\tLoss: 0.264041\n",
            "Train Epoch: 3 [28800/29404 (98%)]\tLoss: 0.274876\n",
            "Accuracy: 0/4861 (0.00%)\n",
            "Train Epoch: 4 [0/29404 (0%)]\tLoss: 0.311946\n",
            "Train Epoch: 4 [640/29404 (2%)]\tLoss: 0.166429\n",
            "Train Epoch: 4 [1280/29404 (4%)]\tLoss: 0.095698\n",
            "Train Epoch: 4 [1920/29404 (7%)]\tLoss: 0.246402\n",
            "Train Epoch: 4 [2560/29404 (9%)]\tLoss: 0.201738\n",
            "Train Epoch: 4 [3200/29404 (11%)]\tLoss: 0.239297\n",
            "Train Epoch: 4 [3840/29404 (13%)]\tLoss: 0.432140\n",
            "Train Epoch: 4 [4480/29404 (15%)]\tLoss: 0.135412\n",
            "Train Epoch: 4 [5120/29404 (17%)]\tLoss: 0.170835\n",
            "Train Epoch: 4 [5760/29404 (20%)]\tLoss: 0.222081\n",
            "Train Epoch: 4 [6400/29404 (22%)]\tLoss: 0.200200\n",
            "Train Epoch: 4 [7040/29404 (24%)]\tLoss: 0.313888\n",
            "Train Epoch: 4 [7680/29404 (26%)]\tLoss: 0.245854\n",
            "Train Epoch: 4 [8320/29404 (28%)]\tLoss: 0.187254\n",
            "Train Epoch: 4 [8960/29404 (30%)]\tLoss: 0.407428\n",
            "Train Epoch: 4 [9600/29404 (33%)]\tLoss: 0.294660\n",
            "Train Epoch: 4 [10240/29404 (35%)]\tLoss: 0.200836\n",
            "Train Epoch: 4 [10880/29404 (37%)]\tLoss: 0.206364\n",
            "Train Epoch: 4 [11520/29404 (39%)]\tLoss: 0.309266\n",
            "Train Epoch: 4 [12160/29404 (41%)]\tLoss: 0.365069\n",
            "Train Epoch: 4 [12800/29404 (43%)]\tLoss: 0.235091\n",
            "Train Epoch: 4 [13440/29404 (46%)]\tLoss: 0.119262\n",
            "Train Epoch: 4 [14080/29404 (48%)]\tLoss: 0.202669\n",
            "Train Epoch: 4 [14720/29404 (50%)]\tLoss: 0.222511\n",
            "Train Epoch: 4 [15360/29404 (52%)]\tLoss: 0.293748\n",
            "Train Epoch: 4 [16000/29404 (54%)]\tLoss: 0.213569\n",
            "Train Epoch: 4 [16640/29404 (57%)]\tLoss: 0.246795\n",
            "Train Epoch: 4 [17280/29404 (59%)]\tLoss: 0.149467\n",
            "Train Epoch: 4 [17920/29404 (61%)]\tLoss: 0.175587\n",
            "Train Epoch: 4 [18560/29404 (63%)]\tLoss: 0.144342\n",
            "Train Epoch: 4 [19200/29404 (65%)]\tLoss: 0.222372\n",
            "Train Epoch: 4 [19840/29404 (67%)]\tLoss: 0.128144\n",
            "Train Epoch: 4 [20480/29404 (70%)]\tLoss: 0.179813\n",
            "Train Epoch: 4 [21120/29404 (72%)]\tLoss: 0.123059\n",
            "Train Epoch: 4 [21760/29404 (74%)]\tLoss: 0.174355\n",
            "Train Epoch: 4 [22400/29404 (76%)]\tLoss: 0.187745\n",
            "Train Epoch: 4 [23040/29404 (78%)]\tLoss: 0.240258\n",
            "Train Epoch: 4 [23680/29404 (80%)]\tLoss: 0.146160\n",
            "Train Epoch: 4 [24320/29404 (83%)]\tLoss: 0.222294\n",
            "Train Epoch: 4 [24960/29404 (85%)]\tLoss: 0.266019\n",
            "Train Epoch: 4 [25600/29404 (87%)]\tLoss: 0.167244\n",
            "Train Epoch: 4 [26240/29404 (89%)]\tLoss: 0.279088\n",
            "Train Epoch: 4 [26880/29404 (91%)]\tLoss: 0.112309\n",
            "Train Epoch: 4 [27520/29404 (93%)]\tLoss: 0.163464\n",
            "Train Epoch: 4 [28160/29404 (96%)]\tLoss: 0.185245\n",
            "Train Epoch: 4 [28800/29404 (98%)]\tLoss: 0.252223\n",
            "Accuracy: 0/4861 (0.00%)\n",
            "Train Epoch: 5 [0/29404 (0%)]\tLoss: 0.135352\n",
            "Train Epoch: 5 [640/29404 (2%)]\tLoss: 0.207099\n",
            "Train Epoch: 5 [1280/29404 (4%)]\tLoss: 0.254013\n",
            "Train Epoch: 5 [1920/29404 (7%)]\tLoss: 0.251024\n",
            "Train Epoch: 5 [2560/29404 (9%)]\tLoss: 0.088162\n",
            "Train Epoch: 5 [3200/29404 (11%)]\tLoss: 0.329440\n",
            "Train Epoch: 5 [3840/29404 (13%)]\tLoss: 0.323373\n",
            "Train Epoch: 5 [4480/29404 (15%)]\tLoss: 0.141422\n",
            "Train Epoch: 5 [5120/29404 (17%)]\tLoss: 0.292085\n",
            "Train Epoch: 5 [5760/29404 (20%)]\tLoss: 0.198493\n",
            "Train Epoch: 5 [6400/29404 (22%)]\tLoss: 0.120458\n",
            "Train Epoch: 5 [7040/29404 (24%)]\tLoss: 0.104279\n",
            "Train Epoch: 5 [7680/29404 (26%)]\tLoss: 0.174877\n",
            "Train Epoch: 5 [8320/29404 (28%)]\tLoss: 0.231299\n",
            "Train Epoch: 5 [8960/29404 (30%)]\tLoss: 0.227271\n",
            "Train Epoch: 5 [9600/29404 (33%)]\tLoss: 0.112875\n",
            "Train Epoch: 5 [10240/29404 (35%)]\tLoss: 0.289649\n",
            "Train Epoch: 5 [10880/29404 (37%)]\tLoss: 0.242598\n",
            "Train Epoch: 5 [11520/29404 (39%)]\tLoss: 0.172430\n",
            "Train Epoch: 5 [12160/29404 (41%)]\tLoss: 0.385154\n",
            "Train Epoch: 5 [12800/29404 (43%)]\tLoss: 0.266206\n",
            "Train Epoch: 5 [13440/29404 (46%)]\tLoss: 0.171342\n",
            "Train Epoch: 5 [14080/29404 (48%)]\tLoss: 0.129005\n",
            "Train Epoch: 5 [14720/29404 (50%)]\tLoss: 0.164128\n",
            "Train Epoch: 5 [15360/29404 (52%)]\tLoss: 0.121975\n",
            "Train Epoch: 5 [16000/29404 (54%)]\tLoss: 0.294605\n",
            "Train Epoch: 5 [16640/29404 (57%)]\tLoss: 0.163081\n",
            "Train Epoch: 5 [17280/29404 (59%)]\tLoss: 0.117563\n",
            "Train Epoch: 5 [17920/29404 (61%)]\tLoss: 0.359842\n",
            "Train Epoch: 5 [18560/29404 (63%)]\tLoss: 0.122695\n",
            "Train Epoch: 5 [19200/29404 (65%)]\tLoss: 0.344296\n",
            "Train Epoch: 5 [19840/29404 (67%)]\tLoss: 0.100430\n",
            "Train Epoch: 5 [20480/29404 (70%)]\tLoss: 0.242403\n",
            "Train Epoch: 5 [21120/29404 (72%)]\tLoss: 0.132338\n",
            "Train Epoch: 5 [21760/29404 (74%)]\tLoss: 0.105301\n",
            "Train Epoch: 5 [22400/29404 (76%)]\tLoss: 0.130981\n",
            "Train Epoch: 5 [23040/29404 (78%)]\tLoss: 0.159987\n",
            "Train Epoch: 5 [23680/29404 (80%)]\tLoss: 0.157775\n",
            "Train Epoch: 5 [24320/29404 (83%)]\tLoss: 0.173343\n",
            "Train Epoch: 5 [24960/29404 (85%)]\tLoss: 0.181212\n",
            "Train Epoch: 5 [25600/29404 (87%)]\tLoss: 0.180051\n",
            "Train Epoch: 5 [26240/29404 (89%)]\tLoss: 0.211207\n",
            "Train Epoch: 5 [26880/29404 (91%)]\tLoss: 0.373927\n",
            "Train Epoch: 5 [27520/29404 (93%)]\tLoss: 0.160153\n",
            "Train Epoch: 5 [28160/29404 (96%)]\tLoss: 0.142928\n",
            "Train Epoch: 5 [28800/29404 (98%)]\tLoss: 0.165504\n",
            "Accuracy: 0/4861 (0.00%)\n",
            "Accuracy: 1584/5139 (30.82%)\n",
            "Accuracy on old subset after training on new subset: 30.823117338003502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfK4Cvprw8k5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}